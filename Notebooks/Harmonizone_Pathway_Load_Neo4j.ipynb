{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,requests,zlib\n",
    "import pandas as pd\n",
    "from py2neo import Graph, NodeMatcher\n",
    "\n",
    "\n",
    "def _download_file(response, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "\n",
    "def _download_and_decompress_file(response, filename):\n",
    "    decompressor = zlib.decompressobj(16 + zlib.MAX_WBITS)\n",
    "    filename = filename[:-3]\n",
    "    with open(filename, 'w+') as f:\n",
    "        while True:\n",
    "            chunk = response.raw.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            string = decompressor.decompress(chunk).decode(\"latin-1\") \n",
    "            f.write(string)\n",
    "\n",
    "def download_datasets(selected_datasets, selected_downloads, decompress=False):\n",
    "    for dataset, path in selected_datasets:\n",
    "        if not os.path.exists(dataset):\n",
    "            os.mkdir(dataset)\n",
    "        for downloadable in selected_downloads:\n",
    "            url = 'https://maayanlab.cloud/static/hdfs/harmonizome/data/%s/%s' %\\\n",
    "                  (path, downloadable)\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = '%s/%s' % (dataset, downloadable)\n",
    "            # Not every dataset has all downloadables.\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            if decompress and 'txt.gz' in filename:\n",
    "                _download_and_decompress_file(response, filename)\n",
    "            else:\n",
    "                _download_file(response, filename)\n",
    "        print('%s downloaded.' % dataset)\n",
    "\n",
    "dwFiles = [\n",
    "           'gene_attribute_edges.txt.gz',\n",
    "           'gene_list_terms.txt.gz',\n",
    "           'attribute_list_entries.txt.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Neo4j has been created with this name\n",
    "dbName = \"PublicOmics\"\n",
    "pw = \"ngs4\"\n",
    "graph = Graph(\"bolt://localhost:7687/\"+dbName, auth=(\"neo4j\", pw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_datasets([('Biocarta Pathways', 'biocarta')], dwFiles, decompress=True)\n",
    "download_datasets([('HumanCyc Pathways', 'humancyc')], dwFiles, decompress=True)\n",
    "download_datasets([('KEGG Pathways', 'kegg')], dwFiles, decompress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways = pd.read_csv('Biocarta Pathways/gene_attribute_edges.txt', sep='\\t')\n",
    "pathways = pathways.drop([\"source_desc\",\"target_desc\"], axis=1)\n",
    "pathways = pathways.iloc[1:]\n",
    "pathways.head(3)\n",
    "for row in pathways.itertuples():\n",
    "    s2 = \"MERGE (g:Gene { Symbol: '%s'}) SET g.EntrezId='%s' \"%(row.source, row.source_id)\n",
    "    s3 = \"MERGE (p:Pathway { Name: '%s', Id: '%s', Origin: 'Biocarta'})\"%(row.target,row.target_id)\n",
    "    r1 = \"MERGE (g)-[:BELONGS_TO{weight:%s}]->(p)\"%(row.weight)\n",
    "    subStmt = \"{} {} {}\".format(s2,s3,r1)\n",
    "    graph.run(subStmt)\n",
    "    \n",
    "pathways = pd.read_csv('HumanCyc Pathways/gene_attribute_edges.txt', sep='\\t')\n",
    "pathways = pathways.drop([\"target_id\",\"target_desc\"], axis=1)\n",
    "pathways = pathways.iloc[1:]\n",
    "pathways.head(3)\n",
    "for row in pathways.itertuples():\n",
    "    s2 = \"MERGE (g:Gene { Symbol: '%s'}) SET g.EntrezId='%s', g.UniprotACC='%s' \"%(row.source, row.source_id, row.source_desc)\n",
    "    s3 = \"MERGE (p:Pathway { Name: '%s', Origin: 'HumanCyc'})\"%(row.target.replace('&','').replace(';','').replace(\"'\",''))\n",
    "    r1 = \"MERGE (g)-[:BELONGS_TO{weight:%s}]->(p)\"%(row.weight)\n",
    "    subStmt = \"{} {} {}\".format(s2,s3,r1)\n",
    "    graph.run(subStmt)\n",
    "    \n",
    "pathways = pd.read_csv('KEGG Pathways/gene_attribute_edges.txt', sep='\\t')\n",
    "pathways = pathways.drop([\"source_desc\",\"target_id\"], axis=1)\n",
    "pathways = pathways.iloc[1:]\n",
    "pathways.head(3)\n",
    "for row in pathways.itertuples():\n",
    "    s2 = \"MERGE (g:Gene {Symbol: '%s'}) SET g.EntrezId='%s'\"%(row.source, row.source_id)\n",
    "    s3 = \"MERGE (p:Pathway {Name: '%s', Id: '%s', Origin:'Kegg'})\"%(row.target,row.target_desc)\n",
    "    r1 = \"MERGE (g)-[:BELONGS_TO{weight:%s}]->(p)\"%(row.weight)\n",
    "    subStmt = \"{} {} {}\".format(s2,s3,r1)\n",
    "    graph.run(subStmt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
